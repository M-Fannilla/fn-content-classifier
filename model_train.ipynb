{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec7b941b7c2f9235",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T17:50:36.325356Z",
     "start_time": "2025-10-21T17:50:36.136716Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0172ef8601fc1e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T17:50:49.673137Z",
     "start_time": "2025-10-21T17:50:37.761577Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \r\n",
      "Aborted!\r\n"
     ]
    }
   ],
   "source": [
    "# Login to wandb\n",
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d130ed3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T17:50:52.160137Z",
     "start_time": "2025-10-21T17:50:52.092107Z"
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "CUDA Version: None\n",
      "PyTorch Version: 2.9.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Import our custom modules\n",
    "from config import Config\n",
    "from dataset import create_data_loaders\n",
    "from metrics import plot_roc_curves, plot_precision_recall_curves\n",
    "from model import create_model, setup_model_for_training, count_parameters\n",
    "from trainer import Trainer\n",
    "\n",
    "\n",
    "def set_seed(seed: int) -> None:\n",
    "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\n",
    "    'cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fe9970c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T17:51:01.786672Z",
     "start_time": "2025-10-21T17:51:01.676320Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  train_size_perc: 0.8\n",
      "  test_size_perc: 0.2\n",
      "  dataset_src: ./fn-content-dataset/compiled\n",
      "  label_dataframe: ./fn-content-dataset/compiled/action_labels.csv\n",
      "  output_dir: ./outputs\n",
      "  seed: 42\n",
      "  batch_size: 32\n",
      "  num_workers: 5\n",
      "  threshold: 0.5\n",
      "  img_size: 224\n",
      "  model_name: convnextv2_tiny\n",
      "  learning_rate: 1e-05\n",
      "  epochs: 20\n",
      "  early_stopping_patience: 10\n",
      "  early_stopping_min_delta: 0.001\n",
      "  lr_reduce_patience: 5\n",
      "  lr_reduce_factor: 0.5\n",
      "  lr_reduce_min_lr: 1e-07\n",
      "  use_wandb: False\n",
      "  wandb_project: convnextv2-multilabel-classification\n",
      "  wandb_entity: None\n",
      "  wandb_run_name: None\n",
      "  wandb_tags: None\n",
      "  model_catalog: {'convnextv2_nano': 224, 'convnextv2_tiny': 224, 'convnextv2_base': 224, 'convnextv2_large': 224, 'convnextv2_huge': 224}\n"
     ]
    }
   ],
   "source": [
    "# Create configuration object\n",
    "config = Config()\n",
    "config.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f434ca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Imbalance Analysis and Configuration\n",
    "# ==========================================\n",
    "\n",
    "print(\"Analyzing class imbalance and configuring class weights...\")\n",
    "\n",
    "# Show current configuration\n",
    "print(f\"\\nClass Weight Configuration:\")\n",
    "print(f\"  Use class weights: {config.use_class_weights}\")\n",
    "print(f\"  Class weight method: {config.class_weight_method}\")\n",
    "print(f\"  Loss function: {config.loss_type}\")\n",
    "\n",
    "# Calculate and display class weights if enabled\n",
    "if config.use_class_weights:\n",
    "    from losses import print_class_weights\n",
    "    \n",
    "    # Get training labels\n",
    "    train_labels = train_loader.dataset.labels\n",
    "    \n",
    "    # Print class weights analysis\n",
    "    class_weights = print_class_weights(\n",
    "        train_labels, \n",
    "        label_columns, \n",
    "        method=config.class_weight_method\n",
    "    )\n",
    "    \n",
    "    # Store class weights for later use\n",
    "    config.class_weights = class_weights\n",
    "    \n",
    "    print(f\"\\nClass weights calculated and stored in config.class_weights\")\n",
    "    print(f\"Class weights shape: {class_weights.shape}\")\n",
    "    print(f\"Class weights device: {class_weights.device if hasattr(class_weights, 'device') else 'CPU'}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nClass weights are disabled. Set config.use_class_weights = True to enable.\")\n",
    "    print(\"Available class weight methods:\")\n",
    "    print(\"  - 'inverse_freq': Inverse frequency weighting (default)\")\n",
    "    print(\"  - 'balanced': Balanced weighting (sklearn style)\")\n",
    "    print(\"  - 'sqrt_inverse_freq': Square root of inverse frequency\")\n",
    "\n",
    "print(f\"\\nAvailable loss functions:\")\n",
    "print(\"  - 'focal': Focal Loss (good for class imbalance)\")\n",
    "print(\"  - 'asymmetric': Asymmetric Loss\")\n",
    "print(\"  - 'weighted_bce': Weighted Binary Cross Entropy\")\n",
    "print(\"  - 'bce': Standard Binary Cross Entropy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdb9b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimenting with Class Weight Configurations\n",
    "# ===============================================\n",
    "\n",
    "print(\"Class Weight Experimentation Options:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n1. To enable/disable class weights:\")\n",
    "print(\"   config.use_class_weights = True   # Enable class weights\")\n",
    "print(\"   config.use_class_weights = False  # Disable class weights\")\n",
    "\n",
    "print(\"\\n2. To change class weight calculation method:\")\n",
    "print(\"   config.class_weight_method = 'inverse_freq'      # Default\")\n",
    "print(\"   config.class_weight_method = 'balanced'          # Sklearn style\")\n",
    "print(\"   config.class_weight_method = 'sqrt_inverse_freq' # Square root\")\n",
    "\n",
    "print(\"\\n3. To change loss function:\")\n",
    "print(\"   config.loss_type = 'focal'        # Focal Loss (recommended for imbalance)\")\n",
    "print(\"   config.loss_type = 'weighted_bce' # Weighted BCE (uses class weights)\")\n",
    "print(\"   config.loss_type = 'asymmetric'   # Asymmetric Loss\")\n",
    "print(\"   config.loss_type = 'bce'          # Standard BCE\")\n",
    "\n",
    "print(\"\\n4. Example configurations for different scenarios:\")\n",
    "print(\"   # For severe class imbalance:\")\n",
    "print(\"   config.use_class_weights = True\")\n",
    "print(\"   config.class_weight_method = 'balanced'\")\n",
    "print(\"   config.loss_type = 'focal'\")\n",
    "print(\"\")\n",
    "print(\"   # For moderate imbalance with weighted BCE:\")\n",
    "print(\"   config.use_class_weights = True\")\n",
    "print(\"   config.class_weight_method = 'inverse_freq'\")\n",
    "print(\"   config.loss_type = 'weighted_bce'\")\n",
    "print(\"\")\n",
    "print(\"   # For no class weighting:\")\n",
    "print(\"   config.use_class_weights = False\")\n",
    "print(\"   config.loss_type = 'focal'  # Still use focal loss for imbalance\")\n",
    "\n",
    "print(\"\\n5. Current configuration:\")\n",
    "print(f\"   use_class_weights: {config.use_class_weights}\")\n",
    "print(f\"   class_weight_method: {config.class_weight_method}\")\n",
    "print(f\"   loss_type: {config.loss_type}\")\n",
    "\n",
    "print(\"\\nNote: Class weights are calculated from training data and applied during training.\")\n",
    "print(\"The weights help the model focus more on underrepresented classes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "783d5a5acc2bd756",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T17:52:49.563323Z",
     "start_time": "2025-10-21T17:52:49.470012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wandb Configuration:\n",
      "  Enabled: True\n",
      "  Project: convnextv2-multilabel-classification\n",
      "  Entity: None\n",
      "  Tags: ['multilabel', 'convnextv2', 'finetuning']\n"
     ]
    }
   ],
   "source": [
    "# Enable wandb logging\n",
    "config.use_wandb = True\n",
    "config.wandb_tags = ['multilabel', 'convnextv2', 'finetuning']\n",
    "config.wandb_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68654e85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T17:59:12.380685Z",
     "start_time": "2025-10-21T17:58:39.187896Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing dataset...\n",
      "Dataset loaded successfully!\n",
      "  Number of classes: 19\n",
      "  Class names: ['69', 'anal_fucking', 'ass_licking', 'ass_penetration', 'fingering', 'grabbing_ass', 'grabbing_boobs', 'grabbing_hair/head', 'handjob', 'kissing', 'masturbation', 'pussy_rubbing', 'vaginal_fucking', 'vaginal_penetration', 'vibrating', 'wet_genitals', 'blowjob', 'cum', 'pussy_licking']\n",
      "  Training samples: 73476\n",
      "  Validation samples: 18386\n",
      "  Training batches: 2297\n",
      "  Validation batches: 575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/milosz/Projects/fn-poc-classifier/venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample batch shape:\n",
      "  Images: torch.Size([32, 3, 224, 224])\n",
      "  Labels: torch.Size([32, 19])\n",
      "  Label range: [0.000, 1.000]\n",
      "  Positive labels per sample: 2.62\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading and preparing dataset...\")\n",
    "\n",
    "# Create data loaders with stratified splitting\n",
    "train_loader, val_loader, label_columns = create_data_loaders(config)\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"  Number of classes: {len(label_columns)}\")\n",
    "print(f\"  Class names: {label_columns}\")\n",
    "print(f\"  Training samples: {len(train_loader.dataset)}\")\n",
    "print(f\"  Validation samples: {len(val_loader.dataset)}\")\n",
    "print(f\"  Training batches: {len(train_loader)}\")\n",
    "print(f\"  Validation batches: {len(val_loader)}\")\n",
    "\n",
    "# Display sample batch info\n",
    "sample_batch = next(iter(train_loader))\n",
    "images, labels = sample_batch\n",
    "print(f\"\\nSample batch shape:\")\n",
    "print(f\"  Images: {images.shape}\")\n",
    "print(f\"  Labels: {labels.shape}\")\n",
    "print(f\"  Label range: [{labels.min():.3f}, {labels.max():.3f}]\")\n",
    "print(f\"  Positive labels per sample: {labels.sum(dim=1).float().mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be8a6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Creation and Setup\n",
    "# =========================\n",
    "\n",
    "print(\"Creating and setting up model...\")\n",
    "\n",
    "# Create model\n",
    "model = create_model(\n",
    "    config=config,\n",
    "    num_classes=len(label_columns)\n",
    ")\n",
    "\n",
    "# Setup model for training (freeze/unfreeze based on training mode)\n",
    "model = setup_model_for_training(\n",
    "    model=model,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Count parameters\n",
    "param_counts = count_parameters(model)\n",
    "print(f\"Model created successfully!\")\n",
    "print(f\"  Total parameters: {param_counts['total']:,}\")\n",
    "print(f\"  Trainable parameters: {param_counts['trainable']:,}\")\n",
    "print(f\"  Frozen parameters: {param_counts['frozen']:,}\")\n",
    "print(f\"  Training mode: Finetuning (backbone frozen, classifier trainable)\")\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "print(f\"  Model moved to: {device}\")\n",
    "\n",
    "# Test forward pass\n",
    "print(\"\\nTesting forward pass...\")\n",
    "with torch.no_grad():\n",
    "    sample_images = torch.randn(2, 3, config.img_size, config.img_size).to(device)\n",
    "    sample_output = model(sample_images)\n",
    "    print(f\"  Input shape: {sample_images.shape}\")\n",
    "    print(f\"  Output shape: {sample_output.shape}\")\n",
    "    print(f\"  Output range: [{sample_output.min():.3f}, {sample_output.max():.3f}]\")\n",
    "\n",
    "# Show class weight configuration\n",
    "print(f\"\\nClass Weight Configuration:\")\n",
    "print(f\"  Use class weights: {config.use_class_weights}\")\n",
    "print(f\"  Class weight method: {config.class_weight_method}\")\n",
    "print(f\"  Loss function: {config.loss_type}\")\n",
    "if hasattr(config, 'class_weights') and config.class_weights is not None:\n",
    "    print(f\"  Class weights calculated: Yes\")\n",
    "    print(f\"  Class weights shape: {config.class_weights.shape}\")\n",
    "else:\n",
    "    print(f\"  Class weights calculated: No\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54dc18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Setup\n",
    "# ==============\n",
    "\n",
    "print(\"Setting up trainer...\")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    config=config,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    label_columns=label_columns,\n",
    "    device=str(device)\n",
    ")\n",
    "\n",
    "print(\"Trainer setup complete!\")\n",
    "print(f\"  Loss function: {config.loss_type}\")\n",
    "print(f\"  Class weights: {'Enabled' if config.use_class_weights else 'Disabled'}\")\n",
    "if config.use_class_weights:\n",
    "    print(f\"  Class weight method: {config.class_weight_method}\")\n",
    "print(f\"  Optimizer: AdamW (lr={config.learning_rate})\")\n",
    "print(f\"  Scheduler: ReduceLROnPlateau (patience={config.lr_reduce_patience})\")\n",
    "print(f\"  Early stopping: Enabled (patience={config.early_stopping_patience})\")\n",
    "print(f\"  Mixed precision: Enabled\")\n",
    "print(f\"  Threshold: {config.threshold}\")\n",
    "print(f\"  Training mode: Finetuning\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21437fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Execution\n",
    "# ==================\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Start training\n",
    "history = trainer.train()\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Training completed!\")\n",
    "print(f\"Best validation F1 Micro: {trainer.best_val_f1:.4f}\")\n",
    "\n",
    "# Plot training history\n",
    "print(\"\\nPlotting training history...\")\n",
    "trainer.plot_training_history(save_path=os.path.join(config.output_dir, 'training_history.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea67004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation and Analysis\n",
    "# =============================\n",
    "\n",
    "print(\"Evaluating model on validation set...\")\n",
    "\n",
    "# Get final validation metrics\n",
    "val_loss, val_metrics = trainer.validate_epoch()\n",
    "\n",
    "print(\"\\nFinal Validation Metrics:\")\n",
    "print(f\"  Loss: {val_loss:.4f}\")\n",
    "print(f\"  F1 Micro: {val_metrics['f1_micro']:.4f}\")\n",
    "print(f\"  F1 Macro: {val_metrics['f1_macro']:.4f}\")\n",
    "print(f\"  F1 Samples: {val_metrics['f1_samples']:.4f}\")\n",
    "print(f\"  Precision Micro: {val_metrics['precision_micro']:.4f}\")\n",
    "print(f\"  Precision Macro: {val_metrics['precision_macro']:.4f}\")\n",
    "print(f\"  Recall Micro: {val_metrics['recall_micro']:.4f}\")\n",
    "print(f\"  Recall Macro: {val_metrics['recall_macro']:.4f}\")\n",
    "\n",
    "if 'roc_auc_micro' in val_metrics:\n",
    "    print(f\"  ROC AUC Micro: {val_metrics['roc_auc_micro']:.4f}\")\n",
    "    print(f\"  ROC AUC Macro: {val_metrics['roc_auc_macro']:.4f}\")\n",
    "    print(f\"  PR AUC Micro: {val_metrics['pr_auc_micro']:.4f}\")\n",
    "    print(f\"  PR AUC Macro: {val_metrics['pr_auc_macro']:.4f}\")\n",
    "\n",
    "# Get per-class metrics\n",
    "print(\"\\nComputing per-class metrics...\")\n",
    "per_class_metrics = trainer.metrics_calculator.compute_per_class_metrics(\n",
    "    y_true=val_loader.dataset.labels,\n",
    "    y_pred=np.concatenate([trainer.model(torch.tensor(batch[0]).to(device)).cpu().detach().numpy()\n",
    "                           for batch in val_loader]),\n",
    "    class_names=label_columns\n",
    ")\n",
    "\n",
    "print(\"\\nPer-class F1 Scores:\")\n",
    "for class_name, metrics in per_class_metrics.items():\n",
    "    print(f\"  {class_name}: {metrics['f1']:.4f} (support: {metrics['support']})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede4f78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization and Analysis\n",
    "# ===========================\n",
    "\n",
    "print(\"Generating visualizations...\")\n",
    "\n",
    "# Get predictions for visualization\n",
    "trainer.model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "all_probabilities = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = trainer.model(images)\n",
    "        probabilities = torch.sigmoid(outputs).cpu().numpy()\n",
    "        predictions = outputs.cpu().numpy()\n",
    "        labels_np = labels.cpu().numpy()\n",
    "\n",
    "        all_predictions.append(predictions)\n",
    "        all_labels.append(labels_np)\n",
    "        all_probabilities.append(probabilities)\n",
    "\n",
    "all_predictions = np.concatenate(all_predictions, axis=0)\n",
    "all_labels = np.concatenate(all_labels, axis=0)\n",
    "all_probabilities = np.concatenate(all_probabilities, axis=0)\n",
    "\n",
    "# Plot ROC curves\n",
    "print(\"Plotting ROC curves...\")\n",
    "plot_roc_curves(\n",
    "    all_labels,\n",
    "    all_probabilities,\n",
    "    label_columns,\n",
    "    save_path=os.path.join(config.output_dir, 'roc_curves.png')\n",
    ")\n",
    "\n",
    "# Plot Precision-Recall curves\n",
    "print(\"Plotting Precision-Recall curves...\")\n",
    "plot_precision_recall_curves(\n",
    "    all_labels,\n",
    "    all_probabilities,\n",
    "    label_columns,\n",
    "    save_path=os.path.join(config.output_dir, 'pr_curves.png')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc0456b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Saving\n",
    "# ============\n",
    "\n",
    "print(\"Saving model and results...\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(config.output_dir, exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "model_path = os.path.join(config.output_dir, f'{config.model_name}_finetuned_best.pth')\n",
    "trainer.save_model(model_path)\n",
    "\n",
    "# Save training history as CSV\n",
    "history_df = pd.DataFrame(history)\n",
    "history_path = os.path.join(config.output_dir, 'training_history.csv')\n",
    "history_df.to_csv(history_path, index=False)\n",
    "print(f\"Training history saved to: {history_path}\")\n",
    "\n",
    "# Save configuration\n",
    "config_path = os.path.join(config.output_dir, 'config.json')\n",
    "import json\n",
    "\n",
    "with open(config_path, 'w') as f:\n",
    "    config_dict = {\n",
    "        'model_name': config.model_name,\n",
    "        'training_mode': 'finetuning',\n",
    "        'learning_rate': config.learning_rate,\n",
    "        'epochs': config.epochs,\n",
    "        'batch_size': config.batch_size,\n",
    "        'img_size': config.img_size,\n",
    "        'threshold': config.threshold,\n",
    "        'label_columns': label_columns,\n",
    "        'best_val_f1': trainer.best_val_f1\n",
    "    }\n",
    "    json.dump(config_dict, f, indent=2)\n",
    "print(f\"Configuration saved to: {config_path}\")\n",
    "\n",
    "print(f\"\\nAll files saved to: {config.output_dir}\")\n",
    "print(\"Training completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c95a568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights & Biases Hyperparameter Sweep\n",
    "# =====================================\n",
    "\n",
    "print(\"Wandb Sweep Options:\")\n",
    "print(\"\\n1. Manual Sweep (Run this notebook multiple times with different configs)\")\n",
    "print(\"   - Modify the config parameters in cell 2\")\n",
    "print(\"   - Run all cells to train with new parameters\")\n",
    "print(\"   - Compare results in wandb dashboard\")\n",
    "\n",
    "print(\"\\n2. Automated Sweep (Recommended)\")\n",
    "print(\"   - Run: python run_sweep.py\")\n",
    "print(\"   - This will automatically test different hyperparameter combinations\")\n",
    "print(\"   - Uses Bayesian optimization to find best parameters\")\n",
    "\n",
    "print(\"\\n3. Sweep Configuration:\")\n",
    "print(\"   - Model: convnextv2_nano, convnextv2_tiny, convnextv2_base\")\n",
    "print(\"   - Learning Rate: 1e-6 to 1e-3 (log scale)\")\n",
    "print(\"   - Batch Size: 16, 32, 64\")\n",
    "print(\"   - Image Size: 224, 256, 384\")\n",
    "print(\"   - Threshold: 0.3, 0.4, 0.5, 0.6, 0.7\")\n",
    "print(\"   - Early Stopping Patience: 5, 10, 15\")\n",
    "print(\"   - LR Reduce Patience: 3, 5, 7\")\n",
    "print(\"   - LR Reduce Factor: 0.3, 0.5, 0.7\")\n",
    "\n",
    "print(\"\\n4. Key Metrics Tracked:\")\n",
    "print(\"   - val_f1_micro (primary metric for optimization)\")\n",
    "print(\"   - val_f1_macro, val_f1_samples\")\n",
    "print(\"   - val_precision_micro, val_precision_macro\")\n",
    "print(\"   - val_recall_micro, val_recall_macro\")\n",
    "print(\"   - val_roc_auc_micro, val_roc_auc_macro\")\n",
    "print(\"   - val_pr_auc_micro, val_pr_auc_macro\")\n",
    "print(\"   - train_loss, val_loss\")\n",
    "print(\"   - learning_rate, best_val_f1\")\n",
    "\n",
    "print(\"\\n5. To start a sweep:\")\n",
    "print(\"   !python run_sweep.py\")\n",
    "print(\"   # Or run the sweep configuration directly:\")\n",
    "print(\"   !wandb sweep wandb_sweep_config.yaml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d437447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping and Learning Rate Reduction\n",
    "# ===========================================\n",
    "\n",
    "print(\"Training Features:\")\n",
    "print(\"\\n1. Early Stopping:\")\n",
    "print(f\"   - Patience: {config.early_stopping_patience} epochs\")\n",
    "print(f\"   - Min Delta: {config.early_stopping_min_delta}\")\n",
    "print(\"   - Monitors: Validation F1 Micro score\")\n",
    "print(\"   - Stops training when no improvement for patience epochs\")\n",
    "\n",
    "print(\"\\n2. Learning Rate Reduction on Plateau:\")\n",
    "print(f\"   - Patience: {config.lr_reduce_patience} epochs\")\n",
    "print(f\"   - Factor: {config.lr_reduce_factor} (reduces LR by this factor)\")\n",
    "print(f\"   - Min LR: {config.lr_reduce_min_lr}\")\n",
    "print(\"   - Monitors: Validation F1 Micro score\")\n",
    "print(\"   - Reduces LR when no improvement for patience epochs\")\n",
    "\n",
    "print(\"\\n3. Benefits:\")\n",
    "print(\"   - Prevents overfitting with early stopping\")\n",
    "print(\"   - Automatically adjusts learning rate for better convergence\")\n",
    "print(\"   - Saves training time by stopping when no improvement\")\n",
    "print(\"   - Always loads the best model at the end\")\n",
    "\n",
    "print(\"\\n4. Monitoring:\")\n",
    "print(\"   - Watch for 'New best F1 Micro' messages\")\n",
    "print(\"   - Watch for 'Reducing learning rate' messages\")\n",
    "print(\"   - Training will stop early if no improvement\")\n",
    "print(\"   - Best model is automatically loaded at the end\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2d5a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Comparison and Experimentation\n",
    "# ====================================\n",
    "\n",
    "print(\"Model comparison and experimentation options:\")\n",
    "print(\"\\n1. Try different ConvNeXt V2 variants:\")\n",
    "print(\"   - convnextv2_nano: Fastest, smallest\")\n",
    "print(\"   - convnextv2_tiny: Good balance (current)\")\n",
    "print(\"   - convnextv2_base: Better accuracy\")\n",
    "print(\"   - convnextv2_large: High accuracy\")\n",
    "print(\"   - convnextv2_huge: Best accuracy, slowest\")\n",
    "\n",
    "print(\"\\n2. Finetuning benefits:\")\n",
    "print(\"   - Fast training (only classifier head is trained)\")\n",
    "print(\"   - Good for small to medium datasets\")\n",
    "print(\"   - Preserves pretrained features\")\n",
    "\n",
    "print(\"\\n3. Try different loss functions:\")\n",
    "print(\"   - Focal Loss: Good for class imbalance (current)\")\n",
    "print(\"   - Asymmetric Loss: Alternative for imbalance\")\n",
    "print(\"   - Weighted BCE: Simple weighted approach\")\n",
    "\n",
    "print(\"\\n4. Hyperparameter tuning suggestions:\")\n",
    "print(\"   - Learning rate: 1e-5 to 1e-3\")\n",
    "print(\"   - Batch size: 16, 32, 64\")\n",
    "print(\"   - Image size: 224, 256, 384\")\n",
    "print(\"   - Threshold: 0.3 to 0.7\")\n",
    "\n",
    "print(\"\\nTo experiment with different models, modify the config in cell 2:\")\n",
    "print(\"config.model_name = 'convnextv2_base'  # Change model\")\n",
    "print(\"config.epochs = 30  # Change epochs\")\n",
    "print(\"config.learning_rate = 2e-5  # Change learning rate\")\n",
    "\n",
    "print(\"\\nTraining completed! Check the outputs directory for results.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
