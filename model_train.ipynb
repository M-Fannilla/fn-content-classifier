{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b60cd9297331650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights & Biases Setup\n",
    "# =====================\n",
    "\n",
    "# Login to wandb (run this cell first)\n",
    "!wandb login\n",
    "\n",
    "# Optional: Set your wandb entity (username/team)\n",
    "# !wandb login --relogin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d130ed3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Import our custom modules\n",
    "from config import Config\n",
    "from dataset import create_data_loaders\n",
    "from metrics import plot_roc_curves, plot_precision_recall_curves\n",
    "from model import create_model, setup_model_for_training, count_parameters\n",
    "from trainer import Trainer\n",
    "\n",
    "\n",
    "def set_seed(seed: int) -> None:\n",
    "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe9970c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configuration object\n",
    "config = Config()\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: {config.model_name}\")\n",
    "print(f\"  Learning Rate: {config.learning_rate}\")\n",
    "print(f\"  Epochs: {config.epochs}\")\n",
    "print(f\"  Batch Size: {config.batch_size}\")\n",
    "print(f\"  Image Size: {config.img_size}\")\n",
    "print(f\"  Device: {device}\")\n",
    "print(f\"  Early Stopping Patience: {config.early_stopping_patience}\")\n",
    "print(f\"  Early Stopping Min Delta: {config.early_stopping_min_delta}\")\n",
    "print(f\"  LR Reduce Patience: {config.lr_reduce_patience}\")\n",
    "print(f\"  LR Reduce Factor: {config.lr_reduce_factor}\")\n",
    "print(f\"  LR Reduce Min LR: {config.lr_reduce_min_lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783d5a5acc2bd756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wandb Configuration for Hyperparameter Sweeping\n",
    "# ===============================================\n",
    "\n",
    "# Uncomment and modify these parameters for hyperparameter sweeping\n",
    "# The sweep will automatically override these values during optimization\n",
    "\n",
    "# config.model_name = 'convnextv2_tiny'  # Options: nano, tiny, base, large, huge\n",
    "# config.learning_rate = 1e-5  # Range: 1e-6 to 1e-3\n",
    "# config.batch_size = 32  # Options: 16, 32, 64\n",
    "# config.img_size = 224  # Options: 224, 256, 384\n",
    "# config.threshold = 0.5  # Range: 0.3 to 0.7\n",
    "# config.early_stopping_patience = 10  # Options: 5, 10, 15\n",
    "# config.lr_reduce_patience = 5  # Options: 3, 5, 7\n",
    "# config.lr_reduce_factor = 0.5  # Options: 0.3, 0.5, 0.7\n",
    "\n",
    "# Enable wandb logging\n",
    "config.use_wandb = True\n",
    "config.wandb_project = 'convnextv2-multilabel-classification'\n",
    "config.wandb_entity = None  # Set to your wandb username/team\n",
    "config.wandb_run_name = None  # Will be auto-generated\n",
    "config.wandb_tags = ['multilabel', 'convnextv2', 'finetuning']\n",
    "\n",
    "print(\"Wandb Configuration:\")\n",
    "print(f\"  Enabled: {config.use_wandb}\")\n",
    "print(f\"  Project: {config.wandb_project}\")\n",
    "print(f\"  Entity: {config.wandb_entity}\")\n",
    "print(f\"  Tags: {config.wandb_tags}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68654e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading and Preparation\n",
    "# ============================\n",
    "\n",
    "print(\"Loading and preparing dataset...\")\n",
    "\n",
    "# Create data loaders with stratified splitting\n",
    "train_loader, val_loader, label_columns = create_data_loaders(config)\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"  Number of classes: {len(label_columns)}\")\n",
    "print(f\"  Class names: {label_columns}\")\n",
    "print(f\"  Training samples: {len(train_loader.dataset)}\")\n",
    "print(f\"  Validation samples: {len(val_loader.dataset)}\")\n",
    "print(f\"  Training batches: {len(train_loader)}\")\n",
    "print(f\"  Validation batches: {len(val_loader)}\")\n",
    "\n",
    "# Display sample batch info\n",
    "sample_batch = next(iter(train_loader))\n",
    "images, labels = sample_batch\n",
    "print(f\"\\nSample batch shape:\")\n",
    "print(f\"  Images: {images.shape}\")\n",
    "print(f\"  Labels: {labels.shape}\")\n",
    "print(f\"  Label range: [{labels.min():.3f}, {labels.max():.3f}]\")\n",
    "print(f\"  Positive labels per sample: {labels.sum(dim=1).float().mean():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be8a6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Creation and Setup\n",
    "# =========================\n",
    "\n",
    "print(\"Creating and setting up model...\")\n",
    "\n",
    "# Create model\n",
    "model = create_model(config, num_classes=len(label_columns))\n",
    "\n",
    "# Setup model for training (freeze/unfreeze based on training mode)\n",
    "model = setup_model_for_training(model, config)\n",
    "\n",
    "# Count parameters\n",
    "param_counts = count_parameters(model)\n",
    "print(f\"Model created successfully!\")\n",
    "print(f\"  Total parameters: {param_counts['total']:,}\")\n",
    "print(f\"  Trainable parameters: {param_counts['trainable']:,}\")\n",
    "print(f\"  Frozen parameters: {param_counts['frozen']:,}\")\n",
    "print(f\"  Training mode: Finetuning (backbone frozen, classifier trainable)\")\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "print(f\"  Model moved to: {device}\")\n",
    "\n",
    "# Test forward pass\n",
    "print(\"\\nTesting forward pass...\")\n",
    "with torch.no_grad():\n",
    "    sample_images = torch.randn(2, 3, config.img_size, config.img_size).to(device)\n",
    "    sample_output = model(sample_images)\n",
    "    print(f\"  Input shape: {sample_images.shape}\")\n",
    "    print(f\"  Output shape: {sample_output.shape}\")\n",
    "    print(f\"  Output range: [{sample_output.min():.3f}, {sample_output.max():.3f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54dc18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Setup\n",
    "# ==============\n",
    "\n",
    "print(\"Setting up trainer...\")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    config=config,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    label_columns=label_columns,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"Trainer setup complete!\")\n",
    "print(f\"  Loss function: Focal Loss\")\n",
    "print(f\"  Optimizer: AdamW (lr={config.learning_rate})\")\n",
    "print(f\"  Scheduler: ReduceLROnPlateau (patience={config.lr_reduce_patience})\")\n",
    "print(f\"  Early stopping: Enabled (patience={config.early_stopping_patience})\")\n",
    "print(f\"  Mixed precision: Enabled\")\n",
    "print(f\"  Threshold: {config.threshold}\")\n",
    "print(f\"  Training mode: Finetuning\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21437fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Execution\n",
    "# ==================\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Start training\n",
    "history = trainer.train()\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Training completed!\")\n",
    "print(f\"Best validation F1 Micro: {trainer.best_val_f1:.4f}\")\n",
    "\n",
    "# Plot training history\n",
    "print(\"\\nPlotting training history...\")\n",
    "trainer.plot_training_history(save_path=os.path.join(config.output_dir, 'training_history.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea67004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation and Analysis\n",
    "# =============================\n",
    "\n",
    "print(\"Evaluating model on validation set...\")\n",
    "\n",
    "# Get final validation metrics\n",
    "val_loss, val_metrics = trainer.validate_epoch()\n",
    "\n",
    "print(\"\\nFinal Validation Metrics:\")\n",
    "print(f\"  Loss: {val_loss:.4f}\")\n",
    "print(f\"  F1 Micro: {val_metrics['f1_micro']:.4f}\")\n",
    "print(f\"  F1 Macro: {val_metrics['f1_macro']:.4f}\")\n",
    "print(f\"  F1 Samples: {val_metrics['f1_samples']:.4f}\")\n",
    "print(f\"  Precision Micro: {val_metrics['precision_micro']:.4f}\")\n",
    "print(f\"  Precision Macro: {val_metrics['precision_macro']:.4f}\")\n",
    "print(f\"  Recall Micro: {val_metrics['recall_micro']:.4f}\")\n",
    "print(f\"  Recall Macro: {val_metrics['recall_macro']:.4f}\")\n",
    "\n",
    "if 'roc_auc_micro' in val_metrics:\n",
    "    print(f\"  ROC AUC Micro: {val_metrics['roc_auc_micro']:.4f}\")\n",
    "    print(f\"  ROC AUC Macro: {val_metrics['roc_auc_macro']:.4f}\")\n",
    "    print(f\"  PR AUC Micro: {val_metrics['pr_auc_micro']:.4f}\")\n",
    "    print(f\"  PR AUC Macro: {val_metrics['pr_auc_macro']:.4f}\")\n",
    "\n",
    "# Get per-class metrics\n",
    "print(\"\\nComputing per-class metrics...\")\n",
    "per_class_metrics = trainer.metrics_calculator.compute_per_class_metrics(\n",
    "    y_true=val_loader.dataset.labels,\n",
    "    y_pred=np.concatenate([trainer.model(torch.tensor(batch[0]).to(device)).cpu().detach().numpy()\n",
    "                           for batch in val_loader]),\n",
    "    class_names=label_columns\n",
    ")\n",
    "\n",
    "print(\"\\nPer-class F1 Scores:\")\n",
    "for class_name, metrics in per_class_metrics.items():\n",
    "    print(f\"  {class_name}: {metrics['f1']:.4f} (support: {metrics['support']})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede4f78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization and Analysis\n",
    "# ===========================\n",
    "\n",
    "print(\"Generating visualizations...\")\n",
    "\n",
    "# Get predictions for visualization\n",
    "trainer.model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "all_probabilities = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = trainer.model(images)\n",
    "        probabilities = torch.sigmoid(outputs).cpu().numpy()\n",
    "        predictions = outputs.cpu().numpy()\n",
    "        labels_np = labels.cpu().numpy()\n",
    "\n",
    "        all_predictions.append(predictions)\n",
    "        all_labels.append(labels_np)\n",
    "        all_probabilities.append(probabilities)\n",
    "\n",
    "all_predictions = np.concatenate(all_predictions, axis=0)\n",
    "all_labels = np.concatenate(all_labels, axis=0)\n",
    "all_probabilities = np.concatenate(all_probabilities, axis=0)\n",
    "\n",
    "# Plot ROC curves\n",
    "print(\"Plotting ROC curves...\")\n",
    "plot_roc_curves(\n",
    "    all_labels,\n",
    "    all_probabilities,\n",
    "    label_columns,\n",
    "    save_path=os.path.join(config.output_dir, 'roc_curves.png')\n",
    ")\n",
    "\n",
    "# Plot Precision-Recall curves\n",
    "print(\"Plotting Precision-Recall curves...\")\n",
    "plot_precision_recall_curves(\n",
    "    all_labels,\n",
    "    all_probabilities,\n",
    "    label_columns,\n",
    "    save_path=os.path.join(config.output_dir, 'pr_curves.png')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc0456b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Saving\n",
    "# ============\n",
    "\n",
    "print(\"Saving model and results...\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(config.output_dir, exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "model_path = os.path.join(config.output_dir, f'{config.model_name}_finetuned_best.pth')\n",
    "trainer.save_model(model_path)\n",
    "\n",
    "# Save training history as CSV\n",
    "history_df = pd.DataFrame(history)\n",
    "history_path = os.path.join(config.output_dir, 'training_history.csv')\n",
    "history_df.to_csv(history_path, index=False)\n",
    "print(f\"Training history saved to: {history_path}\")\n",
    "\n",
    "# Save configuration\n",
    "config_path = os.path.join(config.output_dir, 'config.json')\n",
    "import json\n",
    "\n",
    "with open(config_path, 'w') as f:\n",
    "    config_dict = {\n",
    "        'model_name': config.model_name,\n",
    "        'training_mode': 'finetuning',\n",
    "        'learning_rate': config.learning_rate,\n",
    "        'epochs': config.epochs,\n",
    "        'batch_size': config.batch_size,\n",
    "        'img_size': config.img_size,\n",
    "        'threshold': config.threshold,\n",
    "        'label_columns': label_columns,\n",
    "        'best_val_f1': trainer.best_val_f1\n",
    "    }\n",
    "    json.dump(config_dict, f, indent=2)\n",
    "print(f\"Configuration saved to: {config_path}\")\n",
    "\n",
    "print(f\"\\nAll files saved to: {config.output_dir}\")\n",
    "print(\"Training completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c95a568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights & Biases Hyperparameter Sweep\n",
    "# =====================================\n",
    "\n",
    "print(\"Wandb Sweep Options:\")\n",
    "print(\"\\n1. Manual Sweep (Run this notebook multiple times with different configs)\")\n",
    "print(\"   - Modify the config parameters in cell 2\")\n",
    "print(\"   - Run all cells to train with new parameters\")\n",
    "print(\"   - Compare results in wandb dashboard\")\n",
    "\n",
    "print(\"\\n2. Automated Sweep (Recommended)\")\n",
    "print(\"   - Run: python run_sweep.py\")\n",
    "print(\"   - This will automatically test different hyperparameter combinations\")\n",
    "print(\"   - Uses Bayesian optimization to find best parameters\")\n",
    "\n",
    "print(\"\\n3. Sweep Configuration:\")\n",
    "print(\"   - Model: convnextv2_nano, convnextv2_tiny, convnextv2_base\")\n",
    "print(\"   - Learning Rate: 1e-6 to 1e-3 (log scale)\")\n",
    "print(\"   - Batch Size: 16, 32, 64\")\n",
    "print(\"   - Image Size: 224, 256, 384\")\n",
    "print(\"   - Threshold: 0.3, 0.4, 0.5, 0.6, 0.7\")\n",
    "print(\"   - Early Stopping Patience: 5, 10, 15\")\n",
    "print(\"   - LR Reduce Patience: 3, 5, 7\")\n",
    "print(\"   - LR Reduce Factor: 0.3, 0.5, 0.7\")\n",
    "\n",
    "print(\"\\n4. Key Metrics Tracked:\")\n",
    "print(\"   - val_f1_micro (primary metric for optimization)\")\n",
    "print(\"   - val_f1_macro, val_f1_samples\")\n",
    "print(\"   - val_precision_micro, val_precision_macro\")\n",
    "print(\"   - val_recall_micro, val_recall_macro\")\n",
    "print(\"   - val_roc_auc_micro, val_roc_auc_macro\")\n",
    "print(\"   - val_pr_auc_micro, val_pr_auc_macro\")\n",
    "print(\"   - train_loss, val_loss\")\n",
    "print(\"   - learning_rate, best_val_f1\")\n",
    "\n",
    "print(\"\\n5. To start a sweep:\")\n",
    "print(\"   !python run_sweep.py\")\n",
    "print(\"   # Or run the sweep configuration directly:\")\n",
    "print(\"   !wandb sweep wandb_sweep_config.yaml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d437447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping and Learning Rate Reduction\n",
    "# ===========================================\n",
    "\n",
    "print(\"Training Features:\")\n",
    "print(\"\\n1. Early Stopping:\")\n",
    "print(f\"   - Patience: {config.early_stopping_patience} epochs\")\n",
    "print(f\"   - Min Delta: {config.early_stopping_min_delta}\")\n",
    "print(\"   - Monitors: Validation F1 Micro score\")\n",
    "print(\"   - Stops training when no improvement for patience epochs\")\n",
    "\n",
    "print(\"\\n2. Learning Rate Reduction on Plateau:\")\n",
    "print(f\"   - Patience: {config.lr_reduce_patience} epochs\")\n",
    "print(f\"   - Factor: {config.lr_reduce_factor} (reduces LR by this factor)\")\n",
    "print(f\"   - Min LR: {config.lr_reduce_min_lr}\")\n",
    "print(\"   - Monitors: Validation F1 Micro score\")\n",
    "print(\"   - Reduces LR when no improvement for patience epochs\")\n",
    "\n",
    "print(\"\\n3. Benefits:\")\n",
    "print(\"   - Prevents overfitting with early stopping\")\n",
    "print(\"   - Automatically adjusts learning rate for better convergence\")\n",
    "print(\"   - Saves training time by stopping when no improvement\")\n",
    "print(\"   - Always loads the best model at the end\")\n",
    "\n",
    "print(\"\\n4. Monitoring:\")\n",
    "print(\"   - Watch for 'New best F1 Micro' messages\")\n",
    "print(\"   - Watch for 'Reducing learning rate' messages\")\n",
    "print(\"   - Training will stop early if no improvement\")\n",
    "print(\"   - Best model is automatically loaded at the end\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2d5a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Comparison and Experimentation\n",
    "# ====================================\n",
    "\n",
    "print(\"Model comparison and experimentation options:\")\n",
    "print(\"\\n1. Try different ConvNeXt V2 variants:\")\n",
    "print(\"   - convnextv2_nano: Fastest, smallest\")\n",
    "print(\"   - convnextv2_tiny: Good balance (current)\")\n",
    "print(\"   - convnextv2_base: Better accuracy\")\n",
    "print(\"   - convnextv2_large: High accuracy\")\n",
    "print(\"   - convnextv2_huge: Best accuracy, slowest\")\n",
    "\n",
    "print(\"\\n2. Finetuning benefits:\")\n",
    "print(\"   - Fast training (only classifier head is trained)\")\n",
    "print(\"   - Good for small to medium datasets\")\n",
    "print(\"   - Preserves pretrained features\")\n",
    "\n",
    "print(\"\\n3. Try different loss functions:\")\n",
    "print(\"   - Focal Loss: Good for class imbalance (current)\")\n",
    "print(\"   - Asymmetric Loss: Alternative for imbalance\")\n",
    "print(\"   - Weighted BCE: Simple weighted approach\")\n",
    "\n",
    "print(\"\\n4. Hyperparameter tuning suggestions:\")\n",
    "print(\"   - Learning rate: 1e-5 to 1e-3\")\n",
    "print(\"   - Batch size: 16, 32, 64\")\n",
    "print(\"   - Image size: 224, 256, 384\")\n",
    "print(\"   - Threshold: 0.3 to 0.7\")\n",
    "\n",
    "print(\"\\nTo experiment with different models, modify the config in cell 2:\")\n",
    "print(\"config.model_name = 'convnextv2_base'  # Change model\")\n",
    "print(\"config.epochs = 30  # Change epochs\")\n",
    "print(\"config.learning_rate = 2e-5  # Change learning rate\")\n",
    "\n",
    "print(\"\\nTraining completed! Check the outputs directory for results.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
